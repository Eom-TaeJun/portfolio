{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "19ea97b1",
      "metadata": {
        "id": "19ea97b1"
      },
      "source": [
        "\n",
        "# PPO + Transformer for 3D Box Packing (CPU‑only, Colab‑ready)\n",
        "\n",
        "This notebook implements **Transformer‑based policy** trained with **PPO** for a simplified 3D bin‑packing environment.  \n",
        "It is designed to run **on CPU (no CUDA required)** and is modularized into clear functions.\n",
        "\n",
        "**Key design choices (PPO‑focused):**\n",
        "- Policy predicts **scores over candidate placements** (Categorical over N candidates).  \n",
        "- Proper **PPO returns/advantages (GAE)** with bootstrap value.  \n",
        "- **Reward scaling & clipping** for stable learning.  \n",
        "- **Pre‑LN Transformer**, gradient clipping, and entropy regularization.  \n",
        "- CPU‑only — safe for Colab without GPU setup.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "81c606a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81c606a7",
        "outputId": "6554039d-3a1e-4187-e250-6ae890cb5f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# !pip install numpy pyyaml matplotlib imageio pandas tensorboard\n",
        "\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import yaml\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
        "import imageio\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple, Dict, Any\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "474374d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "474374d5",
        "outputId": "a0e2d71a-342d-4bae-9a17-6b86f52bae50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CONFIG loaded.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "CONFIG = {\n",
        "    \"seed\": 1234,\n",
        "    \"log\": {\"plot_interval\": 20, \"save_dir\": \"/mnt/data/ppo_tf_logs\"},\n",
        "    \"env\": {\n",
        "        \"bin_size\": [25, 32, 50],\n",
        "        \"num_boxes\": 60,\n",
        "        \"min_size\": 4,\n",
        "        \"max_size\": 12,\n",
        "        \"thin_threshold\": 6,\n",
        "        \"max_weight\": 500000,\n",
        "        \"zone_limits\": [50000, 60000, 100000, 50000]\n",
        "    },\n",
        "    \"candidates\": {\"max_N\": 32, \"grid_step\": 3, \"floor_first\": True},\n",
        "    \"reward\": {\n",
        "        \"w_bbox\": 0.8, \"w_contact\": 1.0, \"w_volume\": 3.0,\n",
        "        \"w_wall\": 0.3, \"w_ems\": 0.3, \"w_height\": 0.1,\n",
        "        \"w_layer\": 0.5, \"w_small\": 0.3, \"w_ycenter\": 0.2,\n",
        "        \"w_unload\": 0.2, \"penalty\": 0.5\n",
        "    },\n",
        "    \"reward_norm\": {\"scale\": 5.0, \"clip\": 5.0},\n",
        "    \"ppo\": {\n",
        "        \"gamma\": 0.995, \"lam\": 0.95, \"clip\": 0.15,\n",
        "        \"value_coeff\": 0.5, \"entropy_coeff\": 0.01,\n",
        "        \"lr\": 3e-4, \"max_grad_norm\": 1.0\n",
        "    },\n",
        "    \"train\": {\"episodes\": 200, \"save_gif_every\": 50}\n",
        "}\n",
        "print(\"CONFIG loaded.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "57191578",
      "metadata": {
        "id": "57191578"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def set_seed(seed:int=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "def ensure_dir(path:str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "    return path\n",
        "\n",
        "set_seed(CONFIG[\"seed\"])\n",
        "_ = ensure_dir(CONFIG[\"log\"][\"save_dir\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fe712f9",
      "metadata": {
        "id": "1fe712f9"
      },
      "source": [
        "\n",
        "## Environment (3D bin packing, simplified)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0867cf43",
      "metadata": {
        "id": "0867cf43"
      },
      "outputs": [],
      "source": [
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "@dataclass\n",
        "class EnvConfig:\n",
        "    bin_size: Tuple[int,int,int]\n",
        "    num_boxes: int\n",
        "    min_size: int\n",
        "    max_size: int\n",
        "    thin_threshold: int\n",
        "    max_weight: float\n",
        "    zone_limits: List[float]\n",
        "\n",
        "@dataclass\n",
        "class CandConfig:\n",
        "    max_N: int\n",
        "    grid_step: int\n",
        "    floor_first: bool\n",
        "\n",
        "def sort_boxes_by_priority(boxes):\n",
        "    def priority(b):\n",
        "        areas = sorted([b[0]*b[1], b[1]*b[2], b[0]*b[2]], reverse=True)\n",
        "        return (min(b) < CONFIG[\"env\"][\"min_size\"], -areas[0], -areas[1], -areas[2])\n",
        "    return sorted(boxes, key=priority)\n",
        "\n",
        "def generate_boxes(n, min_s, max_s):\n",
        "    raw = [tuple(np.random.randint(min_s, max_s+1, size=3)) for _ in range(n)]\n",
        "    return sort_boxes_by_priority(raw)\n",
        "\n",
        "def get_orientations(box):\n",
        "    l, w, h = box\n",
        "    return [(l,w,h),(w,l,h),(h,l,w),(l,h,w),(w,h,l),(h,w,l)]\n",
        "\n",
        "def is_collision(pos, size, placed, bin_size):\n",
        "    x,y,z = pos; dx,dy,dz = size\n",
        "    if x+dx > bin_size[0] or y+dy > bin_size[1] or z+dz > bin_size[2]:\n",
        "        return True\n",
        "    for (px,py,pz),(pd,pw,ph) in placed:\n",
        "        if not (x+dx<=px or x>=px+pd or y+dy<=py or y>=py+pw or z+dz<=pz or z>=pz+ph):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def is_stable(pos, size, placed):\n",
        "    x,y,z = pos; dx,dy,dz = size\n",
        "    if z == 0: return True\n",
        "    support=0\n",
        "    for (px,py,pz),(pd,pw,ph) in placed:\n",
        "        if pz+ph == z:\n",
        "            ox = max(0, min(x+dx, px+pd)-max(x, px))\n",
        "            oy = max(0, min(y+dy, py+pw)-max(y, py))\n",
        "            support += ox*oy\n",
        "    return support >= 0.5*dx*dy\n",
        "\n",
        "def get_ems(placed, bin_size):\n",
        "    if not placed: return [(0,0,0)]\n",
        "    ems = set()\n",
        "    for (x,y,z),(dx,dy,dz) in placed:\n",
        "        for off in [(dx,0,0),(0,dy,0),(0,0,dz)]:\n",
        "            pt=(x+off[0], y+off[1], z+off[2])\n",
        "            if 0<=pt[0]<bin_size[0] and 0<=pt[1]<bin_size[1] and 0<=pt[2]<bin_size[2]:\n",
        "                ems.add(pt)\n",
        "    return list(ems)\n",
        "\n",
        "def overlap_with_ems(pos, size, ems):\n",
        "    x,y,z = pos; dx,dy,dz = size\n",
        "    vol=0\n",
        "    for ex,ey,ez in ems:\n",
        "        ox = max(0, min(x+dx, ex)-max(x, ex))\n",
        "        oy = max(0, min(y+dy, ey)-max(y, ey))\n",
        "        oz = max(0, min(z+dz, ez)-max(z, ez))\n",
        "        vol += ox*oy*oz\n",
        "    return vol\n",
        "\n",
        "def generate_candidates(bin_size, placed, box, cand_cfg: CandConfig):\n",
        "    Lx,Ly,Lz = bin_size\n",
        "    step = max(1, cand_cfg.grid_step)\n",
        "    anchors=set()\n",
        "    for x in range(0, Lx, step):\n",
        "        for y in range(0, Ly, step):\n",
        "            anchors.add((x,y,0))\n",
        "    for z in range(0, Lz, max(1, Lz//max(1, (Lz//step)))):\n",
        "        for x in range(0, Lx, step):\n",
        "            anchors.add((x,0,z))\n",
        "        for y in range(0, Ly, step):\n",
        "            anchors.add((0,y,z))\n",
        "    ems = get_ems(placed, bin_size)\n",
        "    anchors |= set(ems)\n",
        "\n",
        "    valid=[]\n",
        "    for a in anchors:\n",
        "        for rot in get_orientations(box):\n",
        "            if is_collision(a, rot, placed, bin_size) or not is_stable(a, rot, placed):\n",
        "                continue\n",
        "            wasted = np.prod(rot) - overlap_with_ems(a, rot, ems)\n",
        "            valid.append((a, rot, wasted))\n",
        "    if not valid:\n",
        "        for _ in range(10):\n",
        "            rx = np.random.randint(0, max(1, Lx-box[0]))\n",
        "            ry = np.random.randint(0, max(1, Ly-box[1]))\n",
        "            pos = (rx, ry, 0)\n",
        "            for rot in get_orientations(box):\n",
        "                if not is_collision(pos, rot, placed, bin_size) and is_stable(pos, rot, placed):\n",
        "                    valid.append((pos, rot, np.prod(rot)))\n",
        "                    break\n",
        "        if not valid:\n",
        "            valid=[((0,0,0), get_orientations(box)[0], float('inf'))]\n",
        "    valid.sort(key=lambda t: t[2])\n",
        "    valid = valid[:cand_cfg.max_N]\n",
        "    feats = np.array([[*p, *r] for p,r,_ in valid], dtype=np.float32)\n",
        "    return valid, feats\n",
        "\n",
        "def compute_reward_components(pos, size, placed, bin_size):\n",
        "    if not placed:\n",
        "        bbox_score = 1.0\n",
        "    else:\n",
        "        xs,ys,zs=[],[],[]\n",
        "        for (x,y,z),(dx,dy,dz) in placed:\n",
        "            xs += [x, x+dx]; ys += [y, y+dy]; zs += [z, z+dz]\n",
        "        vol_box = (max(xs)-min(xs))*(max(ys)-min(ys))*(max(zs)-min(zs))\n",
        "        used    = sum(dx*dy*dz for _,(dx,dy,dz) in placed)\n",
        "        bbox_score = 1.0 - (vol_box - used)/float(np.prod(bin_size))\n",
        "\n",
        "    x0,y0,z0 = pos; dx,dy,dz = size\n",
        "    contact=0.0\n",
        "    if z0==0: contact=1.0\n",
        "    else:\n",
        "        for (px,py,pz),(pd,pw,ph) in placed:\n",
        "            if pz+ph == z0:\n",
        "                ox = max(0, min(x0+dx, px+pd)-max(x0, px))\n",
        "                oy = max(0, min(y0+dy, py+pw)-max(y0, py))\n",
        "                contact += (ox*oy)/(dx*dy+1e-6)\n",
        "        contact = min(1.0, contact)\n",
        "\n",
        "    wall_area=0.0\n",
        "    if x0==0: wall_area += dy*dz\n",
        "    if y0==0: wall_area += dx*dz\n",
        "    if z0==0: wall_area += dx*dy\n",
        "    sa = dx*dy + dy*dz + dx*dz\n",
        "    wall_score = wall_area/(sa+1e-6)\n",
        "\n",
        "    if not placed: height_score=1.0\n",
        "    else:\n",
        "        avg_top = np.mean([z+sz for (x,y,z),(sx,sy,sz) in placed])\n",
        "        height_score = 1.0 - avg_top/bin_size[2]\n",
        "\n",
        "    ems_overlap = overlap_with_ems(pos, size, get_ems(placed, bin_size))/float(np.prod(size)+1e-6)\n",
        "\n",
        "    y_center = y0 + dy/2\n",
        "    y_score  = 1.0 - (y_center/bin_size[1])\n",
        "\n",
        "    return {\"bbox\": bbox_score, \"contact\": contact, \"wall\": wall_score,\n",
        "            \"height\": height_score, \"ems\": ems_overlap, \"ycenter\": y_score}\n",
        "\n",
        "class PackingEnv:\n",
        "    def __init__(self, cfg:EnvConfig, cand_cfg:CandConfig):\n",
        "        self.cfg = cfg\n",
        "        self.cand_cfg = cand_cfg\n",
        "        self.bin_size = tuple(cfg.bin_size)\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.placed: List[Tuple[Tuple[int,int,int], Tuple[int,int,int]]] = []\n",
        "        self.index=0\n",
        "        self.used_volume=0.0\n",
        "        boxes = generate_boxes(self.cfg.num_boxes, self.cfg.min_size, self.cfg.max_size)\n",
        "        self.boxes = sorted(boxes, key=lambda b: b[0]*b[1]*b[2], reverse=True)\n",
        "        return self.boxes[0]\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.index >= len(self.boxes): return None, 0.0, True\n",
        "        pos, rot = action\n",
        "        if is_collision(pos, rot, self.placed, self.bin_size) or not is_stable(pos, rot, self.placed):\n",
        "            r = -CONFIG[\"reward\"][\"penalty\"]\n",
        "            self.index+=1\n",
        "            if self.index>=len(self.boxes): return None, r, True\n",
        "            return self.boxes[self.index], r, False\n",
        "\n",
        "        self.placed.append((pos, rot))\n",
        "        self.index += 1\n",
        "        self.used_volume += float(np.prod(rot))\n",
        "\n",
        "        comps = compute_reward_components(pos, rot, self.placed[:-1], self.bin_size)\n",
        "        vol_ratio = self.used_volume/float(np.prod(self.bin_size))\n",
        "        w = CONFIG[\"reward\"]\n",
        "        raw = (w[\"w_bbox\"]*comps[\"bbox\"] +\n",
        "               w[\"w_contact\"]*comps[\"contact\"] +\n",
        "               w[\"w_wall\"]*comps[\"wall\"] +\n",
        "               w[\"w_height\"]*comps[\"height\"] +\n",
        "               w[\"w_ems\"]*comps[\"ems\"] +\n",
        "               w[\"w_ycenter\"]*comps[\"ycenter\"] +\n",
        "               w[\"w_volume\"]*vol_ratio)\n",
        "        scale = CONFIG[\"reward_norm\"][\"scale\"]\n",
        "        clipv = CONFIG[\"reward_norm\"][\"clip\"]\n",
        "        r = max(-clipv, min(clipv, raw*scale))\n",
        "\n",
        "        if self.index>=len(self.boxes): return None, r, True\n",
        "        return self.boxes[self.index], r, False\n",
        "\n",
        "    def valid_candidates(self, box):\n",
        "        return generate_candidates(self.bin_size, self.placed, box,\n",
        "                                   CandConfig(**CONFIG[\"candidates\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "08870d60",
      "metadata": {
        "id": "08870d60"
      },
      "outputs": [],
      "source": [
        "\n",
        "def draw_bin_with_boxes(bin_size, placed, gif_path):\n",
        "    frames=[]\n",
        "    placed_sorted = sorted(placed, key=lambda b: (b[0][2], b[0][1], b[0][0]))\n",
        "    for i in range(1, len(placed_sorted)+1):\n",
        "        fig = plt.figure(figsize=(5,5))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.set(xlim=(0,bin_size[0]), ylim=(0,bin_size[1]), zlim=(0,bin_size[2]))\n",
        "        for (x,y,z),(dx,dy,dz) in placed_sorted[:i]:\n",
        "            v = [[x,y,z],[x+dx,y,z],[x+dx,y+dy,z],[x,y+dy,z],\n",
        "                 [x,y,z+dz],[x+dx,y,z+dz],[x+dx,y+dy,z+dz],[x,y+dy,z+dz]]\n",
        "            faces = [[v[ii] for ii in face] for face in\n",
        "                     [[0,1,2,3],[4,5,6,7],[0,1,5,4],[2,3,7,6],[1,2,6,5],[0,3,7,4]]]\n",
        "            ax.add_collection3d(Poly3DCollection(faces, facecolors='skyblue', edgecolors='gray', alpha=0.7))\n",
        "        fig.canvas.draw()\n",
        "        w,h = fig.canvas.get_width_height()\n",
        "        buf = np.frombuffer(fig.canvas.tostring_argb(), dtype='uint8').reshape(h, w, 4)[:, :,1:]\n",
        "        frames.append(buf)\n",
        "        plt.close(fig)\n",
        "    imageio.mimsave(gif_path, frames, duration=0.3)\n",
        "    return gif_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ead6dd7",
      "metadata": {
        "id": "7ead6dd7"
      },
      "source": [
        "\n",
        "## Transformer Policy (scores over candidates)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f5ffbf3f",
      "metadata": {
        "id": "f5ffbf3f"
      },
      "outputs": [],
      "source": [
        "\n",
        "class CandidatePolicy(nn.Module):\n",
        "    def __init__(self, d_model=256, nhead=4, nlayers_ctx=2, nlayers_cand=2, pre_ln=True):\n",
        "        super().__init__()\n",
        "        def enc(nl):\n",
        "            layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
        "                                               batch_first=True, norm_first=pre_ln)\n",
        "            return nn.TransformerEncoder(layer, num_layers=nl)\n",
        "\n",
        "        self.env_proj  = nn.Linear(6, d_model)\n",
        "        self.box_proj  = nn.Linear(3, d_model)\n",
        "        self.ctx_enc   = enc(nlayers_ctx)\n",
        "\n",
        "        self.cand_proj = nn.Linear(6, d_model)\n",
        "        self.cand_enc  = enc(nlayers_cand)\n",
        "\n",
        "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
        "        self.score_head  = nn.Sequential(nn.Linear(3*d_model, d_model),\n",
        "                                         nn.ReLU(),\n",
        "                                         nn.Linear(d_model, 1))\n",
        "        self.value_head  = nn.Sequential(nn.Linear(d_model, d_model),\n",
        "                                         nn.ReLU(),\n",
        "                                         nn.Linear(d_model, 1))\n",
        "\n",
        "    def forward(self, env_feat, box_feat, cand_feats):\n",
        "        B,N,_ = cand_feats.shape\n",
        "        ctx_tokens = torch.stack([self.env_proj(env_feat), self.box_proj(box_feat)], dim=1)  # [B,2,d]\n",
        "        H_ctx = self.ctx_enc(ctx_tokens)\n",
        "        h_ctx = H_ctx[:,0]\n",
        "\n",
        "        H_cand = self.cand_enc(self.cand_proj(cand_feats))\n",
        "\n",
        "        q = h_ctx.unsqueeze(1)\n",
        "        h_attn, _ = self.cross_attn(q, H_cand, H_cand)\n",
        "        h_attn = h_attn.squeeze(1)\n",
        "\n",
        "        h_ctx_rep  = h_ctx.unsqueeze(1).expand(B,N,-1)\n",
        "        h_attn_rep = h_attn.unsqueeze(1).expand(B,N,-1)\n",
        "        feats = torch.cat([h_attn_rep*H_cand, h_ctx_rep, H_cand], dim=-1)\n",
        "        scores = self.score_head(feats).squeeze(-1)\n",
        "        value  = self.value_head(h_ctx).squeeze(-1)\n",
        "        return scores, value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9c8aedc",
      "metadata": {
        "id": "d9c8aedc"
      },
      "source": [
        "\n",
        "## PPO Agent (returns, GAE, clipped objective)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "12abefab",
      "metadata": {
        "id": "12abefab"
      },
      "outputs": [],
      "source": [
        "\n",
        "@dataclass\n",
        "class PPOCfg:\n",
        "    gamma: float\n",
        "    lam: float\n",
        "    clip: float\n",
        "    value_coeff: float\n",
        "    entropy_coeff: float\n",
        "    lr: float\n",
        "    max_grad_norm: float\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, policy:CandidatePolicy, cfg:PPOCfg):\n",
        "        self.policy = policy.to(device)\n",
        "        self.cfg = cfg\n",
        "        self.opt = torch.optim.Adam(self.policy.parameters(), lr=cfg.lr)\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_returns_advantages(rewards, values, last_value, gamma, lam):\n",
        "        vals = values + [last_value]\n",
        "        T = len(rewards)\n",
        "        adv, advs = 0.0, [0.0]*T\n",
        "        for t in reversed(range(T)):\n",
        "            delta = rewards[t] + gamma*vals[t+1] - vals[t]\n",
        "            adv = delta + gamma*lam*adv\n",
        "            advs[t] = adv\n",
        "        returns = [advs[t] + vals[t] for t in range(T)]\n",
        "        return torch.tensor(returns, dtype=torch.float32), torch.tensor(advs, dtype=torch.float32)\n",
        "\n",
        "    def select_action(self, env_feat, box_feat, cand_feats):\n",
        "        scores, value = self.policy(env_feat, box_feat, cand_feats)\n",
        "        scores = torch.nan_to_num(scores, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        dist = Categorical(logits=scores)\n",
        "        a = dist.sample()\n",
        "        logp = dist.log_prob(a)\n",
        "        ent = dist.entropy().mean()\n",
        "        return a.item(), logp.squeeze(), ent, value.squeeze()\n",
        "\n",
        "    def update(self, logps, values, advantages, entropies, returns):\n",
        "        advantages = (advantages - advantages.mean())/(advantages.std()+1e-8)\n",
        "        policy_loss = -(torch.stack(logps) * advantages).mean()\n",
        "        value_loss  = F.mse_loss(torch.stack(values).squeeze(), returns)\n",
        "        entropy_loss= torch.stack(entropies).mean()\n",
        "        loss = policy_loss + self.cfg.value_coeff*value_loss - self.cfg.entropy_coeff*entropy_loss\n",
        "        self.opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.cfg.max_grad_norm)\n",
        "        self.opt.step()\n",
        "        return {\"loss\": float(loss.item()), \"policy_loss\": float(policy_loss.item()),\n",
        "                \"value_loss\": float(value_loss.item()), \"entropy\": float(entropy_loss.item())}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "762050fd",
      "metadata": {
        "id": "762050fd"
      },
      "source": [
        "\n",
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fc05c39c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc05c39c",
        "outputId": "82804dfb-0f88-440e-d174-8c618f498499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Ep 0020] R=223.00 | Ent=0.649 | VolUtil=0.680 | score=290.99\n",
            "[Ep 0040] R=201.00 | Ent=0.567 | VolUtil=0.734 | score=274.44\n",
            "[Ep 0060] R=228.50 | Ent=0.499 | VolUtil=0.711 | score=299.55\n",
            "[Ep 0080] R=272.50 | Ent=0.556 | VolUtil=0.689 | score=341.38\n",
            "[Ep 0100] R=239.50 | Ent=0.490 | VolUtil=0.667 | score=306.18\n",
            "[Ep 0120] R=289.00 | Ent=0.325 | VolUtil=0.707 | score=359.65\n",
            "[Ep 0140] R=234.00 | Ent=0.257 | VolUtil=0.626 | score=296.57\n",
            "[Ep 0160] R=300.00 | Ent=0.176 | VolUtil=0.651 | score=365.12\n",
            "[Ep 0180] R=261.50 | Ent=0.108 | VolUtil=0.746 | score=336.10\n",
            "[Ep 0200] R=234.00 | Ent=0.087 | VolUtil=0.601 | score=294.08\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "def env_summary(env:PackingEnv):\n",
        "    vol_util = env.used_volume/float(np.prod(env.bin_size))\n",
        "    placed = len(env.placed)/max(1, env.cfg.num_boxes)\n",
        "    avg_top = 0.0 if not env.placed else np.mean([z+sz for (x,y,z),(sx,sy,sz) in env.placed])/env.bin_size[2]\n",
        "    y_center = 0.0 if not env.placed else np.mean([p[1]+s[1]/2 for p,s in env.placed])/env.bin_size[1]\n",
        "    spare_x = 1.0 - (max([p[0]+s[0] for p,s in env.placed]+[0])/env.bin_size[0])\n",
        "    spare_z = 1.0 - (max([p[2]+s[2] for p,s in env.placed]+[0])/env.bin_size[2])\n",
        "    return np.array([vol_util, placed, avg_top, y_center, spare_x, spare_z], dtype=np.float32)\n",
        "\n",
        "def train():\n",
        "    env_cfg  = EnvConfig(**CONFIG[\"env\"])\n",
        "    cand_cfg = CandConfig(**CONFIG[\"candidates\"])\n",
        "    env = PackingEnv(env_cfg, cand_cfg)\n",
        "\n",
        "    policy = CandidatePolicy(d_model=256, nhead=4, nlayers_ctx=2, nlayers_cand=2, pre_ln=True)\n",
        "    agent  = PPOAgent(policy, PPOCfg(**CONFIG[\"ppo\"]))\n",
        "\n",
        "    reward_log=[]; entropy_log=[]; best = {\"score\": -1e9, \"placed\": []}\n",
        "    episodes = CONFIG[\"train\"][\"episodes\"]\n",
        "    plot_interval = CONFIG[\"log\"][\"plot_interval\"]\n",
        "    save_dir = CONFIG[\"log\"][\"save_dir\"]\n",
        "\n",
        "    last_png_path = None\n",
        "    last_gif_path = None\n",
        "\n",
        "    for ep in range(1, episodes+1):\n",
        "        box = env.reset()\n",
        "        logps=[]; vals=[]; ents=[]; rews=[]\n",
        "        while env.index < env.cfg.num_boxes:\n",
        "            cands, feats = env.valid_candidates(box)\n",
        "            if not cands:\n",
        "                env.index += 1\n",
        "                if env.index >= env.cfg.num_boxes: break\n",
        "                box = env.boxes[env.index]\n",
        "                continue\n",
        "\n",
        "            env_feat = torch.tensor(env_summary(env), dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            box_feat = torch.tensor([box], dtype=torch.float32, device=device)[:, :3]\n",
        "            cand_feats = torch.tensor(feats, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            idx, logp, ent, val = agent.select_action(env_feat, box_feat, cand_feats)\n",
        "\n",
        "            idx = int(np.clip(idx, 0, len(cands)-1))\n",
        "            pos, rot, _ = cands[idx] # Unpack (pos, rot, wasted), ignoring wasted\n",
        "            next_box, r, done = env.step((pos, rot)) # Pass only (pos, rot) to env.step\n",
        "\n",
        "            logps.append(logp); vals.append(val); ents.append(ent); rews.append(float(r))\n",
        "            if done: break\n",
        "            box = next_box\n",
        "\n",
        "        if env.index>=env.cfg.num_boxes or box is None:\n",
        "            last_val = 0.0\n",
        "        else:\n",
        "            env_feat = torch.tensor(env_summary(env), dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            box_feat = torch.tensor([box], dtype=torch.float32, device=device)[:, :3]\n",
        "            dummy = torch.zeros((1,1,6), dtype=torch.float32, device=device)\n",
        "            _, last_val = policy(env_feat, box_feat, dummy)\n",
        "\n",
        "        returns, advs = agent.compute_returns_advantages(rews, [v.item() for v in vals], float(last_val),\n",
        "                                                         CONFIG[\"ppo\"][\"gamma\"], CONFIG[\"ppo\"][\"lam\"])\n",
        "        stats = agent.update(logps, vals, advs, ents, returns)\n",
        "\n",
        "        total_r = sum(rews)\n",
        "        reward_log.append(total_r); entropy_log.append(stats[\"entropy\"])\n",
        "\n",
        "        vol_util = env.used_volume/float(np.prod(env.bin_size))\n",
        "        score = total_r + 100.0*vol_util\n",
        "        if score > best[\"score\"]:\n",
        "            best = {\"score\": score, \"placed\": list(env.placed)}\n",
        "\n",
        "        if ep % plot_interval == 0 or ep == episodes:\n",
        "            print(f\"[Ep {ep:04d}] R={total_r:.2f} | Ent={stats['entropy']:.3f} | VolUtil={vol_util:.3f} | score={score:.2f}\")\n",
        "            plt.figure(figsize=(10,4))\n",
        "            plt.subplot(1,2,1); plt.plot(reward_log); plt.title(\"Reward\"); plt.grid(True)\n",
        "            plt.subplot(1,2,2); plt.plot(entropy_log); plt.title(\"Entropy\"); plt.grid(True)\n",
        "            plt.tight_layout();\n",
        "            png_path = f\"{save_dir}/report_ep{ep}.png\"\n",
        "            plt.savefig(png_path); plt.close()\n",
        "            last_png_path = png_path\n",
        "\n",
        "        if ep % CONFIG[\"train\"][\"save_gif_every\"] == 0 or ep == episodes:\n",
        "            gif_path = f\"{save_dir}/best_ep{ep}.gif\"\n",
        "            draw_bin_with_boxes(env.bin_size, best[\"placed\"], gif_path)\n",
        "            last_gif_path = gif_path\n",
        "\n",
        "    return reward_log, entropy_log, best, last_png_path, last_gif_path\n",
        "\n",
        "reward_log, entropy_log, best, last_png_path, last_gif_path = train()\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7360a40a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7360a40a",
        "outputId": "f9a3fb5f-7614-4ca8-caf2-66e2a1adc39f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved under: /mnt/data/ppo_tf_logs\n",
            "Saved files:\n",
            "/mnt/data/ppo_tf_logs/best_ep100.gif\n",
            "/mnt/data/ppo_tf_logs/best_ep150.gif\n",
            "/mnt/data/ppo_tf_logs/best_ep200.gif\n",
            "/mnt/data/ppo_tf_logs/best_ep50.gif\n",
            "/mnt/data/ppo_tf_logs/report_ep100.png\n",
            "/mnt/data/ppo_tf_logs/report_ep120.png\n",
            "/mnt/data/ppo_tf_logs/report_ep140.png\n",
            "/mnt/data/ppo_tf_logs/report_ep160.png\n",
            "/mnt/data/ppo_tf_logs/report_ep180.png\n",
            "/mnt/data/ppo_tf_logs/report_ep20.png\n",
            "/mnt/data/ppo_tf_logs/report_ep200.png\n",
            "/mnt/data/ppo_tf_logs/report_ep40.png\n",
            "/mnt/data/ppo_tf_logs/report_ep60.png\n",
            "/mnt/data/ppo_tf_logs/report_ep80.png\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"Saved under:\", CONFIG[\"log\"][\"save_dir\"])\n",
        "from glob import glob\n",
        "saved_files = sorted(glob(CONFIG[\"log\"][\"save_dir\"]+\"/*\"))\n",
        "print(\"Saved files:\")\n",
        "for f in saved_files:\n",
        "    print(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSLHP7_qlnXh"
      },
      "id": "HSLHP7_qlnXh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b2d6c8cf",
      "metadata": {
        "id": "b2d6c8cf"
      },
      "source": [
        "\n",
        "### PPO + Transformer Stabilization recap\n",
        "- Candidate categorical action space\n",
        "- Proper GAE with bootstrap\n",
        "- Reward scaling & clipping\n",
        "- Pre‑LN Transformer + grad‑clip + entropy reg\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}